





# Basic libraries
import pandas as pd
import numpy as np

# Datasets
import csv
import json
import string

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns





! ls *zippedData/ *.csv





df_bom = pd.read_csv('zippedData/bom.movie_gross.csv')
df_basics = pd.read_csv('zippedData/name.basics.csv')
df_akas = pd.read_csv('zippedData/title.akas.csv')
df_title_basics = pd.read_csv('zippedData/title.basics.csv')
df_crew = pd.read_csv('zippedData/title.crew.csv')
df_principals = pd.read_csv('zippedData/title.principals.csv')
df_ratings = pd.read_csv('zippedData/title.ratings.csv')
df_movies = pd.read_csv('zippedData/tmdb.movies.csv', index_col=0)
df_movie_budgets = pd.read_csv('zippedData/tn.movie_budgets.csv')

df1 = df_bom.copy()
df2 = df_basics.copy()
df3 = df_akas.copy()
df4 = df_title_basics.copy()
df5 = df_crew.copy()
df6 = df_principals.copy()
df7 = df_ratings.copy()
df8 = df_movies.copy()
df9 = df_movie_budgets.copy()











print(df1.shape)
print()
print(df1.info())
print()


df1.head(2)








print(df2.shape)
print()
print(df2.info())
print()


df2.head(2)








print(df3.shape)
print()
print(df3.info())
print()


df3.head(2)








print(df4.shape)
print()
print(df4.info())
print()


df4.head(2)








print(df5.shape)
print()
print(df5.info())
print()


df5.head(2)








print(df6.shape)
print()
print(df6.info())
print()


df6.head(2)








print(df7.shape)
print()
print(df7.info())
print()


df7.head(2)








print(df8.shape)
print()
print(df8.info())
print()


df8.head(2)








print(df9.shape)
print()
print(df9.info())
print()


df9.head(2)








df9.rename(columns = {'movie':'title'}, inplace=True)
df9.head(1)





DFs = [df1, df2, df3, df4, df5, df6, df7, df8, df9]

for index, DF in enumerate(DFs):
    table_num = index + 1
    print(f'Table {table_num}:-' , sorted(list(DF.columns)), 
          sep = '\n', end = '\n\n')














def column_check(dat1, dat2):
    """
    This function checks to see whether there are columns
    in common. 

    Function counterchecks whether any column in `dat1` has any 
    of its columns in `dat2` columns.
    
    """
    for i in dat1.columns:
        if i in dat2.columns:
            print('Yes, there is a column in common:', i)
        
    else:
        # print('\nNo, Sadly there is no columns are similar.')
        print('\nThe End!')





# Check Columns in common
column_check(df8, df9)


# df8.columns.tolist()
# df9.columns.tolist()


# df8.head(1)
# df9.head(1)

print('The 2 DataFrame have this kind of shapes:-', end = '\n\n')
print('df8 has', df8.shape) # (26517, 9)
print('df9 has', df9.shape) # (5782, 6)


df_89 = df8.merge(df9, how = 'outer', on = 'title', suffixes=('_fr8', '_fr9'))
df_89.head(3)








# df6.head(1)
# df7.head(1)

print('The 2 DataFrame have this kind of shapes:-', end = '\n\n')
print('df6 has', df6.shape) # (1028186, 6)
print('df7 has', df7.shape) # (73856, 3)


# Check Columns in common
column_check(df6, df7)


df_67 = df6.merge(df7, how='outer', on='tconst', suffixes=('_fr6', '_fr7'))
df_67.head(3)





# Check Columns in common
column_check(df5, df_67)


# df5.columns.tolist()
# df5.head(1)
# df67.head(1)

df_567 = df5.merge(df_67, how='outer', on='tconst', suffixes=('_fr5', '_fr67'))
df_567.head(3)





# Check Columns in common
column_check(df4, df_567)





# df4.columns.tolist()
# df4.head(2)

df4.rename(columns={'primary_title':'title'}, inplace = True)

df4.head(3)





# Check Columns in common
column_check(df4, df_567)


# df4.head(1)
# df_567.head(1)

df_4to7 = df4.merge(df_567, how='outer', on='tconst', suffixes=('_fr5', '_fr67'))
df_4to7.head(3)





# Check Columns in common
column_check(df3, df_4to7)


# df3.head(1)
# df_4567.head(1)

df_3to7 = df3.merge(df_4to7, how='outer', on='title', suffixes=('_fr3', '_fr4to7'))
df_3to7.head(3)





# Check Columns in common
column_check(df1, df_3to7)


# df1.head(1)
# df_34567.head(1)

df_1and3to7 = df1.merge(df_3to7, how='outer', on='title', suffixes=('_fr1', '_fr3to7'))
df_1and3to7.head(3)





# Check Columns in common
column_check(df2, df_1and3to7)


df_1to7 = df2.merge(df_1and3to7, how='outer', on='nconst', suffixes=('_fr2', '_fr1_3to7'))
df_1to7.head(3)





# Check Columns in common
column_check(df_1to7, df_89)


df_1to9 = df_1to7.merge(df_89, how='outer', on='title', suffixes=('_fr1to7', '_fr89'))
df_1to9.head(3)


# df_1to9.columns.tolist()


cols_1to9 = [
 # film attributes
 'title_id', 'tconst', 'title', 'is_original_title', 'original_title_fr1to7', 
 'original_title_fr89', 'studio', 'runtime_minutes', 'attributes', 'genres', 'genre_ids', 
 'types', 'category', 'characters', 
 # Timelines
 'year', 'start_year', 'release_date_fr8', 'release_date_fr9',
 # Geo
 'region', 'language', 'original_language',
 # finances
 'production_budget', 'domestic_gross_fr1to7', 'domestic_gross_fr89', 'foreign_gross',
 'worldwide_gross',
 # unknowns
 'id_fr8', 'id_fr9', 'ordering_fr3', 'ordering_fr4to7',
 # professionals
 'nconst', 'primary_name', 'birth_year', 'death_year', 'primary_profession', 'known_for_titles',
 'directors', 'writers', 'job',
 # Popularity scores
 'averagerating', 'numvotes', 'popularity', 'vote_average', 'vote_count'
]

# len(cols_1to9) == len(final_1_cols) 
# print(len(final_1_cols))





df_19 = df_1to9[cols_1to9]
df_19.head()





df_19.head(3)


# df_19.columns.tolist()

df_19.info()





film_attributes = ['title_id', 'tconst', 'title', 'is_original_title', 'original_title_fr1to7', 
 'original_title_fr89', 'studio', 'runtime_minutes', 'attributes', 'genres', 'genre_ids', 
 'types', 'category', 'characters']

df_19[film_attributes].head(3)





# Executable 1
film_attributes = ['title_id', 'tconst', 'title', 'is_original_title', 'studio', 'runtime_minutes', 'attributes', 'genres', 'genre_ids', 
 'types', 'category', 'characters']

# Executable 2
df_19['is_original_title'].replace({0.0: 'No', 1.0: 'Yes', np.nan:'Unknown'}, inplace = True)

df_19[film_attributes].head(3)





shape_0 = df_19.shape

print(f'There are {shape_0} currently in the dataframe.')


# First check how many null values in datasets
nulls_on_runtime = len(df_19.loc[df_19.runtime_minutes.isnull()])
print(f'There are {nulls_on_runtime} null values on the runtime column.') # 518745

# NExt, drop `null` values along the columns
df_19.dropna(subset = ['runtime_minutes'], inplace = True)





shape_1 = df_19.shape

print(f"""There are {shape_1} currently in the dataframe. This is a right after 
dropping all null values on our runtime minutes column.""")


df_19[film_attributes][:3].info()


# Executable 3

print(df_19.runtime_minutes.dtype) 
df_19.runtime_minutes = df_19.runtime_minutes.astype('int64')
# df_19.runtime_minutes.apply(lambda runtime_minutes: runtime_minutes.astype('int64'))


print(f'Now the .dtype has been changed to: {df_19.runtime_minutes.dtype}.') 





# Executable 4
print(f'There are {df_19[film_attributes].genres.isnull().sum()} null values on the genre column.')
print()
print(f'Also, there are {df_19[film_attributes].genre_ids.isnull().sum()} null values on the genre column.')





film_attributes = [ 'title_id', 'tconst', 'title', 'is_original_title', 'studio', 
                   'runtime_minutes', 'attributes', 'genres', 'types', 'category', 
                   'characters']





timelines_geo_cols = [
 # Timelines
 'year', 'start_year', 'release_date_fr8', 'release_date_fr9',
 # Geo
 'region', 'language', 'original_language'
]


df_19[timelines_geo_cols].head(3)


df_19[timelines_geo_cols].info()





# Executable 1
print(df_19.year.dtype)
print(df_19.start_year.dtype)


# df_19.loc[df_19.year.isnull()]
print(f"""The number of null values along the `year` column are {df_19.year.isnull().sum()}.""", 
      end ='\n\n')

print(f'Also, the number of null values along the `start_year` column are {df_19.start_year.isnull().sum()}.')








# checking whether we have a null value in this column
np.nan in df_19.start_year.unique()


df_19.start_year = df_19.start_year.astype('int64')

print(f'Now, we have:-')
print(df_19.start_year.dtype)


df_19[timelines_geo_cols].head(3)








# Executable 2

timelines_geo_cols = [
 # Timelines
 'start_year', 'release_date_fr8',
 # Geo
 'region', 'language', 'original_language'
]


# Executable 3

nulls_in_lang = df_19[timelines_geo_cols].language.unique()
no_nulls_in_lang = len(df_19[timelines_geo_cols].language.unique())

print(f'{nulls_in_lang} They are {no_nulls_in_lang} in number.', end = '\n\n')

nulls_in_orig_lang = df_19[timelines_geo_cols].original_language.unique()
no_nulls_in_orig_lang = len(df_19[timelines_geo_cols].original_language.unique())

print(f"""{nulls_in_orig_lang} They are {no_nulls_in_orig_lang} in number.""", end = '\n\n')





# Checking to see how many null values are 
# present in each of these cols

print(df_19.language.isnull().sum())
print(df_19.original_language.isnull().sum())





timelines_geo_cols = [
 # Timelines
 'start_year', 'release_date_fr8',
 # Geo
 'region', 'original_language'
]


df_19[timelines_geo_cols].head()





finances = [
 'production_budget', 'domestic_gross_fr1to7', 'domestic_gross_fr89', 
 'foreign_gross', 'worldwide_gross'
]


df_19[finances].info()





# executable 1 and 2
finances = [
 'production_budget', 'domestic_gross_fr89', 'worldwide_gross'
]





# # Executable 3
# def monetize(item):
#     if item != np.nan:
#         return int(item[1:].replace(',',''))
    
# df_19.production_budget = df_19.production_budget.map(monetize)
# df_19.domestic_gross = df_19.domestic_gross.map(monetize)
# df_19.worldwide_gross = df_19.worldwide_gross.map(monetize)


df_19[finances].head(3)








unknowns = [
 # unknowns
 'id_fr8', 'id_fr9', 'ordering_fr3', 'ordering_fr4to7',
 # # professionals
 # 'nconst', 'primary_name', 'birth_year', 'death_year', 'primary_profession', 'known_for_titles',
 # 'directors', 'writers', 'job',
 # # Popularity scores
 # 'averagerating', 'numvotes', 'popularity', 'vote_average', 'vote_count',
]


df_19[unknowns].head(3)








professionals = [
 'nconst', 'primary_name', 'birth_year', 'death_year', 'primary_profession', 'known_for_titles',
 'directors', 'writers', 'job'
]


df_19[professionals].head(3)





# check whether there are null values in 'birth_year' and 'death year'

np.nan in df_19.birth_year.unique()
np.nan in df_19.death_year.unique()


df_19.birth_year








popularity_scores = [
 'averagerating', 'numvotes', 'popularity', 'vote_average', 'vote_count'
]


df_19[popularity_scores].head()


df_19[popularity_scores].info()





# the final dataset is:-

popularity_scores = [
 'averagerating', 'numvotes', 'popularity'
]





final_cols = [
 # film_attributes
 'title_id', 'tconst', 'title', 'is_original_title', 'studio', 'runtime_minutes',
 'attributes', 'genres', 'types', 'category', 'characters',
 # Timelines
 'start_year', 'release_date_fr8',
 # Geo
 'region', 'original_language',
 # finances
 'production_budget', 'domestic_gross_fr89', 'worldwide_gross',
 # professionals
 'nconst', 'primary_name', 'birth_year', 'death_year', 'primary_profession', 'known_for_titles',
 'directors', 'writers', 'job',
 # popularity_scores
 'averagerating', 'numvotes', 'popularity'
]


# A glance at what we have now
final_df = df_19[final_cols].copy()
final_df.head(1)





# Follow-up on Suggestions
# (i)
# changing the column name of `domestic_gross_fr89`
final_df.rename(columns={'domestic_gross_fr89': 'domestic_gross'}, inplace = True)

# Checking where the type has been made
'domestic_gross' in final_df.columns


# Follow-up on Suggestions
# (ii) Changing the datatypes of the financial columns
final_df['production_budget'] = final_df.production_budget.replace('[\$,]', '', regex=True).astype(float)
final_df['domestic_gross'] = final_df['domestic_gross'].replace('[\$,]', '', regex=True).astype(float)
final_df['worldwide_gross'] = final_df['worldwide_gross'].replace('[\$,]', '', regex=True).astype(float)


# Basic information on columns
final_df.info()


# Basic information on columns
final_df.describe()





# Export the DataFrame
# final_df.to_csv('final_df.csv')


# > Commented out since it generate a `csv` that is quite sizable


# Checking if it has been written on disk
# 'final_df.csv' in (! ls *.csv)
! ls *final_df.csv





df = final_df.copy()
df.head(3)


# Finding the number of items in df's columns
# df.columns

df = df[['title', 'is_original_title', 'studio', 'runtime_minutes', 
       'genres', 'start_year', 'original_language','region', 
       'directors', 'writers', 'production_budget', 'domestic_gross',  
       'worldwide_gross', 'averagerating', 'numvotes', 'popularity']]

df.head(3)


df.info()


df.describe()








print(df.columns)

# titles in DF
# len(df.title) # 1830683

# unique titles
df.title.nunique() #107459

# no of repeated titles
# len(df.title) - df.title.nunique()





# number of original titles
original_ser = df.groupby('is_original_title').is_original_title.count()#.to_frame()
Orig_Titles = pd.DataFrame(original_ser).rename(columns = {'is_original_title': 'Count'})
x = list(Orig_Titles.values)
x

Orig_Titles.values


# df.groupby(['title', 'production_budget', 'domestic_gross']).value_counts().to_frame()[:2]
df[['title','is_original_title','production_budget', 'domestic_gross', 'worldwide_gross']].groupby('title').count().sort_values(by=['production_budget', 'domestic_gross'], ascending = False)[:15]





# number of unique studios
df.studio.nunique()

# list of unique studios
df.studio.unique()


# # df.groupby(['title', 'production_budget', 'domestic_gross']).value_counts().to_frame()[:2]
df[['title','studio', 'region','production_budget', 'domestic_gross', 'worldwide_gross']].groupby('title').count(
).sort_values(by=['production_budget', 'domestic_gross'], ascending = False)[:15]





print(df.genres.unique())

# Best performing genres by gross
df.groupby(['genres', 'domestic_gross', 'worldwide_gross']
          ).genres.value_counts().to_frame().sort_values(
        by=['domestic_gross'], ascending = False)[:10]





df.runtime_minutes.to_frame().sort_values(by=['runtime_minutes'], ascending = False)


# Best performing genres by gross
df.groupby(['runtime_minutes', 'domestic_gross', 'worldwide_gross']).genres.value_counts(
).to_frame().sort_values(by=['domestic_gross'], ascending = False)[:10]


top_10_avg_runtime_df = df.groupby(['runtime_minutes', 'title', 'domestic_gross', 'worldwide_gross']).count().sort_values(by='worldwide_gross', ascending = False)

top_10_avg_runtime_df = top_10_avg_runtime_df.reset_index()
top_10_avg_runtime_df.runtime_minutes.mean()





# How many unique region do we have?
print(df.region.nunique())

# Which are the unique regions?
df.region.unique()


df[['region', 'title', 'studio', 'genres','production_budget', 'domestic_gross', 
    'worldwide_gross']].groupby('region').count().sort_values(by=['worldwide_gross', 
    'worldwide_gross'], ascending = False)[:15]





# how many unique languages in the dataset
print(df.original_language.nunique())

# what are the unique languages in the dataset
df.original_language.unique()


df[['title', 'studio', 'region', 'genres', 'original_language', 'production_budget', 'domestic_gross', 
    'worldwide_gross']].groupby('original_language').count().sort_values(by=['worldwide_gross', 
    'worldwide_gross'], ascending = False)[:15]
